{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2c48c03b94470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def custom_breakpointhook(*args, **kwargs):\n",
    "    from IPython.core.debugger import set_trace; set_trace(*args, **kwargs)\n",
    "sys.breakpointhook = custom_breakpointhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "        \n",
    "if os.environ['USER'] == \"dsweet2\":\n",
    "    add(\"/Users/dsweet2/Projects/yuchen/auction-gym/src\")\n",
    "else:\n",
    "    add(\"/Users/yuchenji/PycharmProjects/auction-gym/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27158bad55c0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import parse_config, instantiate_agents, instantiate_auction\n",
    "from ad_auction import AdAuction\n",
    "from bo_bidder import BOBidder\n",
    "from const_shading_bidder import ConstShadingBidder\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ad_auction\n",
    "%aimport bo_bidder\n",
    "%aimport const_shading_bidder\n",
    "%aimport main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0300f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ad_auction, num_iterations):\n",
    "    trace = []\n",
    "    for _ in range(num_iterations):\n",
    "        r = ad_auction.run_episode()\n",
    "        trace.append(r)\n",
    "    return np.array(trace).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9db36d",
   "metadata": {},
   "source": [
    "# Advantage-E\n",
    "\n",
    "Bidders get an advantage when they shade their bids, i.e. when they bid a little lower than the ad is really worth, because they save money if they win the auction. If they bid *too* low, however, they lose the auction, which is bad because they want the ad slot.\n",
    "\n",
    "With `config-advantage-E.json` our agent shades it bid w/`EmpiricalShadedBidder` and the other agents bid the true cost (`TruthfulBidder`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_auction = AdAuction(\"configs/config-advantage-E.json\", warm_up_iterations=1000)\n",
    "print(evaluate(ad_auction, num_iterations=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d49e7",
   "metadata": {},
   "source": [
    "The number printed above is our average reward for an episode.\n",
    "\n",
    "[Actually, it's called \"return\" when talking about an episode. One step -- in this case, an auction -- yields a reward. When you take many steps in sequence, receiving a reward for each step, you can sum (or average) of the rewards and call it \"return\".]\n",
    "\n",
    "In any case, the *return*, here, is `[our net utility - mean(other agent's net utility)] / (mean gross utility)` averaged over the `num_iterations` simulated auctions. The mean gross utility is a mean over all of the agents (including us). It's a measure of the total value generated by the auction -- the total revenue that's up for grabs. That value (i.e., revenue) is split between the bidders and the company that runs the auction.\n",
    "\n",
    "Since the number returned by `evaluate()` is compared to the other agents in the auctions, we'llrefer to it as our *advantage*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1545754",
   "metadata": {},
   "source": [
    "# TruthfulBidder vs. TO/EO/VO\n",
    "\n",
    "Now it gets interesting. We bid truthfully (`TruthfulBidder`), and the *other* agents bid either:\n",
    "\n",
    "- TO: Truthfully, also\n",
    "- EO: Shading with `EmpiricalShadedBidder`\n",
    "- VO: Shading with `ValueLearningBidder`\n",
    "\n",
    "In the first case, TO, we receive exactly zero advantage. The actual number shown below for TO is non-zero b/c the auction process is noisy. That's why we need to run multiple times -- and that's why real auction experiments take so long to run. (And, guess what, Bayesian optimization was made for problems where evaluation takes a long time and is noisy.)\n",
    "\n",
    "In the other two cases, EO and VO, we have negative advantage. It's better to shade your bid in a smart way than to bid full price all the time.\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "Notice the argument `warm_up_iterations`. The tells `AdAuction` how many times to run the auction before doing any evaluation at all. During the warm-up time the learning agents (EO and VO) get a chance to learn about the dynamics of the auction. The auction simulator is interesting in that the agents are aware of each others' behavior via the auction, so they are all learning simultaneously, making bids, and learning even more from observing others' bids. If you look at the plots at the bottom of `auction-gym/src\n",
    "/Getting Started with AuctionGym (2. Effects of Bid Shading).ipynb` you'll see that it can take a few iterations --each of which consists of many auction rounds -- before the plots settle down. Called a transient, it's there because the agents take time to learn.\n",
    "\n",
    "We want to do our evaluations after the transient. That will simulate an engineer creating a new ad-bidding bot and trying to optimize it via experiment in an already-functioning ad auction market. Also, it's after the transient that the other agents are at their best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttype in [\"TO\", \"EO\", \"VO\"]:\n",
    "    ad_auction = AdAuction(f\"configs/config-{ttype}.json\",  warm_up_iterations=1000)\n",
    "    print (ttype, evaluate(ad_auction, num_iterations=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf27c8",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "The goal is to optimize a bidder with Bayesian optimization. To do that we need:\n",
    "\n",
    "1. A way to simulate a sequence of experiments. Done. See above.\n",
    "2. A parameterized agent.\n",
    "3. A Bayesian optimizer.\n",
    "\n",
    "\n",
    "This week, work on step 2.\n",
    "\n",
    "## BOBidder\n",
    "\n",
    "We want to optimize parameters by observing rewards (or, in our case, \"advantages\"). When you optimize this way in RL, it's called *policy search*. If you look in the auction-gym codebase you'll find `PolicyLearningBidder`. That will be our starting point for BOBidder.\n",
    "\n",
    "To get started, set up an evaluation of a hacked copy of `PolicyLearningBidder`:\n",
    "\n",
    "- Make a copy of `PolicyLearningBidder` a new file (`bo_bidder.py`) in your Capstone directy.\n",
    "- Completely remove the method `update()` -- even the signature. You can also remove the code that is referred to as \"Option 1\". We're only going to use \"Option 2\". Also, get rid of any references to `gamma` or any other variables that you're not using. `self.model` will be doing the bulk of the work.\n",
    "- Copy `configs/config-advantage-E.json` to `configs/config-advantage-BO.json`. Inside, replace the `EmpiricalShadedBidder` with `BOBidder`.\n",
    "- Try to get `evaluate()` to run on your new config file.\n",
    "\n",
    "It probably won't perform well, but that's because it's not optimized. The first step is just getting it to run.\n",
    "\n",
    "Next, you'll need to figure our how to get and set the parameters. Create two methods in `BOBidder`:\n",
    "- `get_parameters(self) -> np.ndarray`, and\n",
    "- `set_parameters(self, parameters: np.ndarray)`\n",
    "\n",
    "and make them do the right things.  Fortunately, I have some code lying around that could help. See `ParametersDirect` in `parameters_direct.py`. It should help you get and set the parameters of `self.model`, which is a PyTorch module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfdd33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d288de",
   "metadata": {},
   "source": [
    "# BOBidder Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7e468",
   "metadata": {},
   "source": [
    "Can we instantiate it and generate a bid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aceaf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(17)\n",
    "bob = BOBidder(rng)\n",
    "bob.bid(1, None, .123)   # BOBidder does not use context. Maybe later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baaa003",
   "metadata": {},
   "source": [
    "Can we run auctions with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_classes={'BOBidder': BOBidder}\n",
    "\n",
    "ad_auction = AdAuction(\n",
    "    f\"configs/config-advantage-BO.json\",  warm_up_iterations=10,\n",
    "    extra_classes=extra_classes  # Let auction_gym know about our new bidder class\n",
    ")\n",
    "evaluate(ad_auction, num_iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba39318",
   "metadata": {},
   "source": [
    "Now run a full-scale auction evaluation. Our BOBidder will run with randomly-initialized model parameters, and the rest of the bidders will be TruthfulBidders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea239ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_auction = AdAuction(\"configs/config-advantage-BO.json\", warm_up_iterations=1000, extra_classes=extra_classes)\n",
    "print(evaluate(ad_auction, num_iterations=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53ae09",
   "metadata": {},
   "source": [
    "Apparently it's even better to shade your bid randomly than to bid truthfully!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c97e7",
   "metadata": {},
   "source": [
    "## BOBidder vs Constant Bidder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf379373",
   "metadata": {},
   "source": [
    "Now create a new class, `ConstShadingBidder` (see `const_shading_bidder.py`), that just shading by a randomly-chosen constant, `gamma`.\n",
    "\n",
    "We'll have one `BOBidder` compete against a bunch of `ConstShadingBidder`s, and we'll track the advantage of the `BOBidder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_classes={'BOBidder': BOBidder, 'ConstShadingBidder': ConstShadingBidder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3595e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_auction = AdAuction(\n",
    "    \"configs/config-BO-vs-CO.json\",\n",
    "    warm_up_iterations=1,\n",
    "    extra_classes=extra_classes,  # We need to tell auction_gym about our new bidders\n",
    "    seed=17\n",
    ")\n",
    "print(evaluate(ad_auction, num_iterations=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc0dab",
   "metadata": {},
   "source": [
    "Is the result consistent, even though we don't have a fixed seed (i.e., seed='random')?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c075392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(ad_auction, num_iterations=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e33409",
   "metadata": {},
   "source": [
    "Yes. They're both around -0.24\n",
    "With the same set of competitors (the 10 ConstShadingBidder agents) and the same BOBidder, we get approximately the same result, even though the auction process has randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8ee87",
   "metadata": {},
   "source": [
    "# Optimize it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a756841",
   "metadata": {},
   "source": [
    "To optimize our `BOBidder` we need to be able to:\n",
    "\n",
    "- Propose parameters\n",
    "- Set the parameters in BOBidder\n",
    "- Evalute BOBidder with the parameters\n",
    "\n",
    "You've already implemented the last two steps. We'll implement the first as just random parameter selection to get started. Later, the first step will be a Bayesian optimizer.\n",
    "\n",
    "First, create an auction, find the `BOBidder`, and display its current parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17933395",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_auction = AdAuction(\n",
    "    \"configs/config-BO-vs-CO.json\",\n",
    "    warm_up_iterations=1,\n",
    "    extra_classes=extra_classes,  # We need to tell auction_gym about our new bidders\n",
    "    seed=17\n",
    ")\n",
    "\n",
    "bo_bidder = ad_auction.us().bidder\n",
    "print (\"PARAMETERS:\", bo_bidder.get_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a8a79",
   "metadata": {},
   "source": [
    "Next, set the parameters to random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = len(bo_bidder.get_parameters())\n",
    "bo_bidder.set_parameters( .1*np.random.normal(size=(num_params,)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72931c70",
   "metadata": {},
   "source": [
    "Now evaluate the `BOBidder` with the randomly-chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e14343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(ad_auction, num_iterations=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f491182",
   "metadata": {},
   "source": [
    "To demonstrate optimization, we'll write a simple optimizer that just randomly pertubs the parameters over and over and keeps track of which parameter set performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of our simple optimizer\n",
    "num_rounds = 100\n",
    "eps = .1\n",
    "\n",
    "# Initialize the parameters, x_best, to all zeros.\n",
    "\n",
    "x_best = np.zeros(shape=(num_params,))\n",
    "y_best = -1e99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d43313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"num_params = {num_params}\")\n",
    "x = x_best\n",
    "for _ in range(num_rounds):\n",
    "    # Evaluate the current x\n",
    "    bo_bidder.set_parameters(x)\n",
    "    y = evaluate(ad_auction, num_iterations=1000)\n",
    "    \n",
    "    # Keep track of the best so far\n",
    "    if y > y_best:\n",
    "        y_best = y\n",
    "        x_best = x\n",
    "    \n",
    "    # Propose a new x that is a small perturbation\n",
    "    #  of the best x.\n",
    "    x = x_best + eps*np.random.normal(size=(num_params,))\n",
    "    print (f\"EVAL: y_best = {y_best:.4f} y = {y:.4f} x = {x[0]:.2f}, {x[1]:.2f}, ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b3b71",
   "metadata": {},
   "source": [
    "That's not bad for optimization by random perturbations.\n",
    "Can we do better with Bayesian optimization? We'll find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a99965",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "Please read through SKOpt's Bayesian optimization library [documentation](https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html) and see if you can apply `gp_minimize()` to this problem.\n",
    "\n",
    "NB: The algorithm `gp_minimize()` minimizes -- hence the name :) -- but we want to *maximize* the output of `evaluate()`, so just pass the arithmetic inverse of the advantage, `-evaluate()`, to `gp_minimize()`, and it'll perform a maximization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9ca63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf499c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
