{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from main import parse_config, instantiate_agents, instantiate_auction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse configuration file\n",
    "rng, config, agent_configs, agents2items, agents2item_values,\\\n",
    "num_runs, max_slots, embedding_size, embedding_var,\\\n",
    "obs_embedding_size = parse_config('../auction-gym/config/SP_Oracle.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444da1a7",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "For an explanation of the config fields, see [CONFIG.md](https://github.com/amzn/auction-gym/blob/065f8bf325ebbec9c96631625ef1c36df3870cb3/CONFIG.md?plain=1#L30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c92dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = instantiate_agents(rng, agent_configs, agents2item_values, agents2items)\n",
    "\n",
    "auction, num_iter, rounds_per_iter, output_dir =\\\n",
    "    instantiate_auction(rng,\n",
    "                        config,\n",
    "                        agents2items,\n",
    "                        agents2item_values,\n",
    "                        agents,\n",
    "                        max_slots,\n",
    "                        embedding_size,\n",
    "                        embedding_var,\n",
    "                        obs_embedding_size)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3476ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's decrease rounds_per_iter for now b/c we just want to test things out.\n",
    "# Using a lower rounds_per_iter just means that the runs will take less time.\n",
    "rounds_per_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba964ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(rounds_per_iter):\n",
    "    auction.simulate_opportunity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents[0].net_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d286a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a5821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dba884d",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "The paper mentions (in the conclusion) that the bandit model that they study is a limiting approximation of the full bidding problem\n",
    "\n",
    "Bandit models are myopic: They associate the outcome of a single auction with their bid in that auction, which is good, but they ignore the fact that their bid in this auction will impact what other bidders learn from the auction which, in turn will impact how they bid in subsequent auctions. And all of the other bidders’ bids will affect what we learn, etc.\n",
    "\n",
    "All of that feedback means that your bid now will affect not only this auction but future auctions, too. So that outcome of an auction should not only be associated with your bid in that single auction, but also in previous auctions. But how? It’s tough. In reinforcement learning this is called the credit assignment problem.\n",
    "Episodic policy search is non-myopic and makes explicit credit assignment unnecessary.\n",
    "Bayesian optimization is a good way to do episodic policy search in a simulation *and* in real, production, advertising systems.\n",
    "\n",
    "See paragraph below eqn (14):\n",
    "\n",
    "Finally, note that these measures are only well-defined in the bandit-based setting where we can easily characterise the theoretically optimal bidding strategy. When moving to full reinforcement learning scenarios, this will no longer be the case. Indeed, when cur- rent actions influence future states, this adds significant complexity to the problem setting, obscuring the notion of optimality.\n",
    "\n",
    "To understand what a “bidder” does in the AuctionGym simulation, look at TruthfulBidder\n",
    "https://github.com/amzn/auction-gym/blob/065f8bf325ebbec9c96631625ef1c36df3870cb3/src/Bidder.py#L28C16-L28C16\n",
    "\n",
    "The bid() method returns the price an advertiser will pay for a click on the ad (called value, ex., value = $3/click) times the probability that the user will click on that ad (called estimated_CTR, ex., estimated_CTR=0.01). The returned number is the expected dollar revenue of showing the ad:\n",
    "\n",
    "```\n",
    "  E[$revenue] = P{user will click on the ad} * [$amount advertiser will pay for the click]\n",
    "  E[$revenue] = estimated_CTR * value\n",
    "```\n",
    "\n",
    "See notes at https://github.com/amzn/auction-gym/blob/065f8bf325ebbec9c96631625ef1c36df3870cb3/src/Auction.py#L64\n",
    "The TruthfulBidder says, “I will bid — in the ad auction — the actual expected value for this ad.”\n",
    "```\n",
    "bid = E[$revenue]\n",
    "```\n",
    "The learning bidders are more clever: They try to win while still bidding a little lower than the true expected value, because, why not? They save money. The paper and code refer to this as “bid shading”.\n",
    "\n",
    "```\n",
    "bid = gamma*E[$revenue], where gamma, the shading amount, is in [0,1]\n",
    "```\n",
    "\n",
    "The shading value is a function of value and estimated_CTR.\n",
    "\n",
    "Look at method PolicyLearningBidder.bid()\n",
    "https://github.com/amzn/auction-gym/blob/065f8bf325ebbec9c96631625ef1c36df3870cb3/src/Bidder.py#L348\n",
    "\n",
    "PolicyLearningBidder uses as its model BidShadingContextualBandit\n",
    "See BidShadingContextualBandit at https://github.com/amzn/auction-gym/blob/065f8bf325ebbec9c96631625ef1c36df3870cb3/src/Models.py#L93\n",
    "\n",
    "BidShadingContextualBandit is a PyTorch nn.Module. As such it has parameters that describe the function that maps x to gamma.\n",
    "\n",
    "PolicyLearningBidder.bid() makes a feature vector, x, from (value, estimated_CTR)\n",
    "https://github.com/amzn/auction-gym/blob/065f8bf325ebbec9c96631625ef1c36df3870cb3/src/Bidder.py#L360C53-L360C53\n",
    "\n",
    "The model (BidShadingContextualBandit.forward()) maps the feature vector x to gamma. It also produces something called a propensity value, but we can discuss that later if you’re not already familiar with it.\n",
    "\n",
    "Finally, PolicyLearningBidder.bid() returns gamma*value*estimated_CTR, the shaded bid value.\n",
    "In a second-price auction, the bidder who bids the highest price wins, but *pays* the bid of the second-highest bidder. So if you bid $3.00 and I bid $2.50, you win the auction and get to show your ad, but you pay only $2.50. \n",
    "\n",
    "This is great… except:\n",
    "Next time I might want to bid $3.01 to win, thus driving up the price of this ad slot.\n",
    "As a rule, you want to bid as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f0af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
